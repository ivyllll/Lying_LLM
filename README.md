# Lying_LLM

4/22;

Fetch the activations of all specific topics based on three prompts (honest, neurtral, deceptive).

Run **step1** to examine the probing accuracy on several topics (on truthful prompt).

4/23;

Continue to run **step1** for deceptive and neutral prompts.

Get the pkl file for all three prompts, containing the average accuracies and the standard deviations of two probing methods TTPD and LR among all layers.

Run **step2**:

**layer 15:**
| Prompt Type | Probe   | Mean Accuracy (%) | Std. Dev (%) |
|-------------|---------|-------------------|--------------|
| Truthful    | TTPD    | 79.99              | 0.42         |
|             | LRProbe | 78.12              | 2.76         |
| Deceptive   | TTPD    | 80.36              | 0.14         |
|             | LRProbe | 68.36              | 5.25         |
| Neutral     | TTPD    | 84.88              | 0.46         |
|             | LRProbe | 75.43              | 4.53         |

4/26ï¼›

Add SAE on the collected activations to do further analysis:

1. Feature Shift Analysis for truthful vs neutral / truthful vs deceptive / neutral vs deceptive
        using Cosine Similarity, Overlap Ratio, and L2 score to evaluate the tendency through layers
2. 


4/27ï¼›
**step2: **
| Layer | Truthful-TTPD (Mean Â± Std) | Truthful-LR (Mean Â± Std) | Neutral-TTPD (Mean Â± Std) | Neutral-LR (Mean Â± Std) | Deceptive-TTPD (Mean Â± Std) | Deceptive-LR (Mean Â± Std) |
|:-----:|:--------------------------:|:-----------------------:|:--------------------------:|:-----------------------:|:--------------------------:|:-----------------------:|
| 0 | 50.69 Â± 1.10 | 49.50 Â± 0.00 | 50.43 Â± 0.65 | 49.50 Â± 0.00 | 50.63 Â± 1.00 | 49.50 Â± 0.00 |
| 1 | 50.17 Â± 0.35 | 49.56 Â± 0.35 | 50.36 Â± 0.66 | 49.48 Â± 0.11 | 50.16 Â± 0.36 | 49.48 Â± 0.17 |
| 2 | 50.07 Â± 0.08 | 49.64 Â± 0.27 | 50.21 Â± 0.19 | 49.79 Â± 0.43 | 50.20 Â± 0.36 | 49.76 Â± 0.45 |
| 3 | 50.23 Â± 0.31 | 50.62 Â± 0.88 | 50.49 Â± 0.50 | 50.74 Â± 0.96 | 50.14 Â± 0.24 | 50.11 Â± 0.68 |
| 4 | 49.83 Â± 0.63 | 51.20 Â± 0.64 | 49.88 Â± 0.40 | 51.08 Â± 0.83 | 49.91 Â± 0.56 | 50.95 Â± 0.62 |
| 5 | 49.91 Â± 0.55 | 50.92 Â± 0.18 | 49.88 Â± 0.47 | 50.93 Â± 0.28 | 49.87 Â± 0.24 | 50.84 Â± 0.33 |
| 6 | 52.38 Â± 1.31 | 52.02 Â± 0.64 | 51.58 Â± 1.48 | 52.63 Â± 1.06 | 51.01 Â± 1.70 | 52.52 Â± 1.12 |
| 7 | 52.90 Â± 1.26 | 52.13 Â± 0.61 | 52.04 Â± 0.86 | 52.94 Â± 1.02 | 52.46 Â± 0.75 | 52.12 Â± 0.67 |
| 8 | 58.66 Â± 1.96 | 53.09 Â± 1.60 | 57.93 Â± 1.99 | 56.53 Â± 3.05 | 57.77 Â± 2.41 | 53.88 Â± 2.34 |
| 9 | 59.58 Â± 4.57 | 61.37 Â± 3.67 | 57.23 Â± 4.96 | 62.20 Â± 4.01 | 58.79 Â± 3.42 | 62.05 Â± 3.08 |
| 10 | 68.41 Â± 0.74 | 57.16 Â± 3.11 | 64.66 Â± 2.49 | 59.57 Â± 3.65 | 60.11 Â± 3.96 | 57.07 Â± 3.38 |
| 11 | 69.69 Â± 0.63 | 63.11 Â± 3.09 | 69.75 Â± 0.83 | 62.33 Â± 4.54 | 69.13 Â± 0.73 | 58.76 Â± 3.67 |
| 12 | 77.20 Â± 0.23 | 65.37 Â± 4.95 | 80.10 Â± 0.24 | 62.92 Â± 5.64 | 73.43 Â± 0.83 | 62.25 Â± 4.66 |
| 13 | 78.22 Â± 0.23 | 75.70 Â± 2.88 | 82.25 Â± 0.53 | 71.66 Â± 4.21 | 79.29 Â± 0.30 | 70.44 Â± 3.74 |
| 14 | 80.04 Â± 0.47 | 75.97 Â± 3.46 | 84.86 Â± 0.47 | 75.49 Â± 3.53 | 80.42 Â± 0.13 | 70.07 Â± 6.07 |
| 15 | 82.58 Â± 0.60 | 77.15 Â± 4.43 | 85.62 Â± 0.14 | 74.91 Â± 4.07 | 79.23 Â± 0.11 | 71.33 Â± 4.35 |
| 16 | 83.62 Â± 0.41 | 76.97 Â± 3.27 | 85.42 Â± 0.15 | 74.03 Â± 5.79 | 79.80 Â± 0.29 | 69.46 Â± 6.76 |
| 17 | 83.55 Â± 0.35 | 74.53 Â± 4.70 | 85.01 Â± 0.30 | 74.43 Â± 4.92 | 76.25 Â± 0.35 | 67.11 Â± 6.53 |
| 18 | 83.24 Â± 0.22 | 73.62 Â± 5.45 | 83.75 Â± 0.34 | 73.71 Â± 4.70 | 78.69 Â± 0.18 | 67.65 Â± 5.26 |
| 19 | 83.07 Â± 0.41 | 74.90 Â± 4.18 | 83.62 Â± 0.22 | 71.85 Â± 4.34 | 78.69 Â± 0.48 | 66.16 Â± 7.26 |
| 20 | 82.84 Â± 0.27 | 69.61 Â± 5.96 | 83.41 Â± 0.36 | 76.20 Â± 4.87 | 79.50 Â± 0.34 | 66.87 Â± 6.48 |
| 21 | 82.53 Â± 0.35 | 71.29 Â± 5.17 | 83.61 Â± 0.21 | 73.31 Â± 5.80 | 79.22 Â± 0.23 | 66.44 Â± 7.18 |
| 22 | 82.25 Â± 0.29 | 73.93 Â± 5.95 | 83.39 Â± 0.24 | 72.20 Â± 4.89 | 79.06 Â± 0.37 | 67.68 Â± 6.19 |
| 23 | 82.11 Â± 0.37 | 71.52 Â± 5.36 | 83.22 Â± 0.24 | 72.48 Â± 6.43 | 78.54 Â± 0.33 | 68.94 Â± 5.61 |
| 24 | 82.10 Â± 0.29 | 73.07 Â± 6.04 | 83.30 Â± 0.33 | 72.60 Â± 6.11 | 78.24 Â± 0.19 | 67.35 Â± 6.50 |
| 25 | 81.96 Â± 0.29 | 71.62 Â± 6.31 | 83.26 Â± 0.19 | 73.14 Â± 5.08 | 78.33 Â± 0.26 | 70.58 Â± 4.26 |
| 26 | 81.55 Â± 0.28 | 71.34 Â± 6.16 | 82.97 Â± 0.23 | 73.90 Â± 5.83 | 77.45 Â± 0.56 | 69.06 Â± 5.07 |
| 27 | 81.42 Â± 0.33 | 73.38 Â± 5.56 | 82.90 Â± 0.24 | 75.66 Â± 5.13 | 76.81 Â± 0.30 | 68.20 Â± 5.64 |
| 28 | 81.36 Â± 0.18 | 75.02 Â± 3.82 | 82.70 Â± 0.25 | 73.47 Â± 4.71 | 76.43 Â± 0.38 | 68.37 Â± 6.25 |
| 29 | 81.31 Â± 0.22 | 70.84 Â± 6.26 | 82.87 Â± 0.34 | 72.73 Â± 4.14 | 76.51 Â± 0.60 | 69.63 Â± 7.97 |
| 30 | 81.62 Â± 0.23 | 71.57 Â± 5.66 | 82.89 Â± 0.33 | 71.96 Â± 5.07 | 76.17 Â± 0.71 | 63.63 Â± 7.32 |
| 31 | 81.57 Â± 0.33 | 64.56 Â± 4.66 | 83.87 Â± 0.19 | 72.36 Â± 5.25 | 77.86 Â± 1.23 | 68.55 Â± 5.39 |

## ğŸ” Project Summary: Probing and Visualizing Internal Representations of LLaMA

### ğŸ”— Function Call Overview

```text
main()
â”œâ”€â”€ if run_step["step1"]:
â”‚   â””â”€â”€ run_step1(...)             # Train & evaluate TTPD & LR probes on 12 topic-specific datasets
â”‚       â””â”€â”€ collect_training_data(...)   # Load activations, labels, polarities from local storage
â”‚       â””â”€â”€ TTPD.from_data(...), LRProbe.from_data(...)  # Fit probes
â”‚       â””â”€â”€ DataManager.add_dataset(...) â†’ probe.pred(...)  # Evaluate probes on held-out datasets
â”‚       â””â”€â”€ compute_statistics(...), compute_average_accuracies(...)
â”‚       â””â”€â”€ Save accuracy summary as chat_probe_accuracies_layerwise.pkl
â”‚
â”œâ”€â”€ visualize_layerwise_probe_accuracy()  # Plot probe accuracy curves across all layers (TTPD vs LR)
â”‚   â””â”€â”€ Load chat_probe_accuracies_layerwise.pkl â†’ generate line plot â†’ save as PNG
â”‚
â”œâ”€â”€ for layer in range(0, 32):
â”‚   â””â”€â”€ visualize_latent_space(...)       # Project and plot activations at a fixed layer (13/15/32)
â”‚       â””â”€â”€ collect_training_data(...)    # Load activations
â”‚       â””â”€â”€ t-SNE / UMAP / PCA / Isomap â†’ save 2D plots as PNG
â”‚
â””â”€â”€ if run_step["step2"]:
    â””â”€â”€ run_step2(...)             # Evaluate probes on logical conjunctions/disjunctions
        â””â”€â”€ Similar to run_step1, but different test sets
```


### ğŸ“Œ Function Descriptions

- **main()**: The main entrypoint to run experiments, visualization, or evaluations.
- **run_step1(...)**: Train and cross-validate TTPD/LR probes on factual datasets.
- **run_step2(...)**: Evaluate generalization of probes to logical combinations (conj/disj).
- **visualize_layerwise_probe_accuracy()**: Visualize probing accuracy across all layers.
- **visualize_latent_space(...)**: Visualize activations (layer 12) in 2D using four projection methods.

---

## ğŸ” é¡¹ç›®æ€»ç»“ï¼šä½¿ç”¨æ¢é‡å™¨åˆ†æ LLaMA å†…éƒ¨è¡¨å¾

### ğŸ”— å‡½æ•°è°ƒç”¨è·¯å¾„

```text
main()
â”œâ”€â”€ if run_step["step1"]:
â”‚   â””â”€â”€ run_step1(...)             # åŸ¹è®­ TTPD å’Œ LR æ¢é‡å™¨ï¼Œå¯¹ 12 ä¸ªä¸»é¢˜è¿›è¡Œåˆ†ç±»æ·»åŠ 
â”‚       â””â”€â”€ collect_training_data(...)   # ä»æœ¬åœ°æ–‡ä»¶è¯»å–æ¿€æ´»å€¼ã€æ ‡ç­¾ã€ææ€§
â”‚       â””â”€â”€ TTPD.from_data(...), LRProbe.from_data(...)  # è°ƒç”¨æ¢é‡å™¨è¿›è¡Œè®­ç»ƒ
â”‚       â””â”€â”€ DataManager.add_dataset(...) â†’ probe.pred(...)  # åœ¨ç•™å‡ºé›†è¿›è¡Œåˆ†ç±»æµ‹è¯•
â”‚       â””â”€â”€ compute_statistics(...), compute_average_accuracies(...)
â”‚       â””â”€â”€ å°†æ·»åŠ ç»“æœä¿å­˜ä¸º chat_probe_accuracies_layerwise.pkl
â”‚
â”œâ”€â”€ visualize_layerwise_probe_accuracy()  # åŒæ—¶ç»˜åˆ¶ TTPD å’Œ LR æ¢é‡ç±»å‹çš„å±‚çº§åˆ†ç±»ç²¾åº¦æ›²çº¿
â”‚   â””â”€â”€ è¯»å– chat_probe_accuracies_layerwise.pkl è¿›è¡Œç»˜å›¾å¹¶ä¿å­˜ PNG
â”‚
â”œâ”€â”€ for layer in range(0, 32):
â”‚   â””â”€â”€ visualize_latent_space(...)       # åœ¨ç»™å®šå±‚ (13/15/32) å¯¹æ¿€æ´»å€¼è¿›è¡Œç¼©çº¿æˆ 2D è¿›è¡Œå¯è§†åŒ–
â”‚       â””â”€â”€ collect_training_data(...)    # è¯»å–æ•°æ®
â”‚       â””â”€â”€ t-SNE / UMAP / PCA / Isomap è¿›è¡Œé™ç»´ç»˜å›¾
â”‚
â””â”€â”€ if run_step["step2"]:
    â””â”€â”€ run_step2(...)             # è¯„ä¼° TTPD / LR åœ¨é€»è¾‘å…³ç³»æ•°æ®é›†ä¸Šçš„è¿åŒ–æ€§
        â””â”€â”€ å’Œ run_step1 ç±»ä¼¼ï¼Œä½†æ˜¯æµ‹è¯•é›†ä¸åŒ
```


### ğŸ“Œ å‡½æ•°ä»‹ç»

- **main()**: ç¨‹åºè¿›å…¥ç‚¹ï¼Œç”¨äºè¿è¡Œå®éªŒã€åå¤„ç†ã€æˆ–è¡¨å¾å¯è§†åŒ–
- **run_step1(...)**: å¯¹ TTPD å’Œ LR æ¢é‡å™¨è¿›è¡Œåˆ†ç±»è®­ç»ƒï¼Œé‡å¤äº¤å‰éªŒè¯
- **run_step2(...)**: è¯„ä¼°æ¢é‡å™¨å¯¹é€»è¾‘å½¢å¼ç»Ÿè®¡å€¼çš„è¿åŒ–æ€§
- **visualize_layerwise_probe_accuracy()**: æ¼”ç¤ºå„å±‚æ¢é‡ç±»å‹çš„åˆ†ç±»ç²¾åº¦è¾ƒåŠ¨å›¾
- **visualize_latent_space(...)**: å¯¹ä¸€å±‚çš„æ¿€æ´»å€¼è¿›è¡Œ t-SNE/UMAP/PCA/Isomap é™ç»´å’Œç»˜å›¾

