{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up\n",
    "General installs, device setup and load models (LLM and SAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  # for google colab users\n",
    "    import google.colab # type: ignore\n",
    "    from google.colab import output\n",
    "    COLAB = True\n",
    "    %pip install sae-lens transformer-lens\n",
    "except:\n",
    "  # for local setup\n",
    "    COLAB = False\n",
    "    from IPython import get_ipython # type: ignore\n",
    "    ipython = get_ipython(); assert ipython is not None\n",
    "    ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
    "    ipython.run_line_magic(\"autoreload\", \"2\")\n",
    "\n",
    "# Imports for displaying vis in Colab / notebook\n",
    "import webbrowser\n",
    "import http.server\n",
    "import socketserver\n",
    "import threading\n",
    "PORT = 8000\n",
    "\n",
    "# general imports\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import plotly.express as px\n",
    "\n",
    "torch.set_grad_enabled(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# package import\n",
    "from torch import Tensor\n",
    "from transformer_lens import utils\n",
    "from functools import partial\n",
    "from jaxtyping import Int, Float\n",
    "import torch\n",
    "\n",
    "# device setup\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_vis_inline(filename: str, height: int = 850):\n",
    "    '''\n",
    "    Displays the HTML files in Colab. Uses global `PORT` variable defined in prev cell, so that each\n",
    "    vis has a unique port without having to define a port within the function.\n",
    "    '''\n",
    "    if not(COLAB):\n",
    "        webbrowser.open(filename);\n",
    "\n",
    "    else:\n",
    "        global PORT\n",
    "\n",
    "        def serve(directory):\n",
    "            os.chdir(directory)\n",
    "\n",
    "            # Create a handler for serving files\n",
    "            handler = http.server.SimpleHTTPRequestHandler\n",
    "\n",
    "            # Create a socket server with the handler\n",
    "            with socketserver.TCPServer((\"\", PORT), handler) as httpd:\n",
    "                print(f\"Serving files from {directory} on port {PORT}\")\n",
    "                httpd.serve_forever()\n",
    "\n",
    "        thread = threading.Thread(target=serve, args=(\"/content\",))\n",
    "        thread.start()\n",
    "\n",
    "        output.serve_kernel_port_as_iframe(PORT, path=f\"/{filename}\", height=height, cache_in_notebook=True)\n",
    "\n",
    "        PORT += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model and pretrained SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "blocks.2.hook_resid_pre\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/xianxuan_sae/lib/python3.10/site-packages/sae_lens/sae.py:146: UserWarning: \n",
      "This SAE has non-empty model_from_pretrained_kwargs. \n",
      "For optimal performance, load the model like so:\n",
      "model = HookedSAETransformer.from_pretrained_no_processing(..., **cfg.model_from_pretrained_kwargs)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "from sae_lens import SAE\n",
    "# from sae_lens.toolkit.pretrained_saes import get_gpt2_res_jb_saes\n",
    "\n",
    "# Choose a layer you want to focus on\n",
    "# For this tutorial, we're going to use layer 2\n",
    "layer = 2\n",
    "\n",
    "# get model\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\", device = device)\n",
    "\n",
    "# get the SAE for this layer\n",
    "sae, cfg_dict, _ = SAE.from_pretrained(\n",
    "    release = \"gpt2-small-res-jb\",\n",
    "    sae_id = f\"blocks.{layer}.hook_resid_pre\",\n",
    "    device = device\n",
    ")\n",
    "\n",
    "# get hook point\n",
    "hook_point = sae.cfg.hook_name\n",
    "print(hook_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine the feature of interest and the index\n",
    "Here set \"Jedi\" as the simple token prompt and try to steer a \"Jedi\" feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 768])\n",
      "torch.return_types.topk(\n",
      "values=tensor([[[107.2709, 105.1811,  96.7976],\n",
      "         [ 27.9996,   8.2975,   5.5113],\n",
      "         [ 16.7904,  10.1490,   9.2543]]], device='cuda:0'),\n",
      "indices=tensor([[[ 1151, 10488,  3344],\n",
      "         [17972,  9293, 23888],\n",
      "         [ 7650,   718, 22372]]], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "sv_prompt = \"Jedi\"\n",
    "sv_logits, cache = model.run_with_cache(sv_prompt, prepend_bos=True)\n",
    "# print(cache[hook_point].shape) # [1, 3, 768] bs, seq_len, d_model\n",
    "\n",
    "tokens = model.to_tokens(sv_prompt)\n",
    "\n",
    "# print(tokens) # ['<|endoftext|>', 'J', 'edi']\n",
    "# print(model.to_str_tokens(sv_prompt))\n",
    "\n",
    "# get the feature activations from our SAE\n",
    "sv_feature_acts = sae.encode(cache[hook_point])\n",
    "# print(sv_feature_acts.shape)  # [1, 3, 24576] bs, seq_len, d_sae\n",
    "# print(sv_feature_acts) # as we can see the most elements are 0s\n",
    "\n",
    "# get sae_out\n",
    "sae_out = sae.decode(sv_feature_acts)\n",
    "\n",
    "# print out the top activations, focus on the indices\n",
    "print(torch.topk(sv_feature_acts, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768, 24576])\n",
      "torch.Size([24576, 768])\n"
     ]
    }
   ],
   "source": [
    "print(sae.W_enc.shape)\n",
    "print(sae.W_dec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement steering vector and affect the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steering_vector = sae.W_dec[7650] # \n",
    "\n",
    "example_prompt = \"What do we find in space?\"\n",
    "coeff = 100\n",
    "sampling_kwargs = dict(temperature=1.0, top_p=0.1, freq_penalty=1.0) \n",
    "# only select the top_p generated tokenes\n",
    "# Reduce redundant phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up hook functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steering_hook(resid_pre, hook):\n",
    "    \"\"\"\n",
    "    resid_pre: Residual stream activation of one layer, with the shape [bs, seq_len, d_model]\n",
    "\n",
    "    \"\"\"\n",
    "    # when the seq_len is 1, there is no previous context to lead the generation\n",
    "    if resid_pre.shape[1] == 1:  \n",
    "        return\n",
    "\n",
    "    # the current pos\n",
    "    position = sae_out.shape[1]\n",
    "    if steering_on:\n",
    "      # using our steering vector and applying the coefficient\n",
    "      # conduct on the activations of the tokens that are already generated\n",
    "      resid_pre[:, :position - 1, :] += coeff * steering_vector\n",
    "\n",
    "\n",
    "def hooked_generate(prompt_batch, fwd_hooks=[], seed=None, **kwargs):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "    with model.hooks(fwd_hooks=fwd_hooks):\n",
    "        tokenized = model.to_tokens(prompt_batch)\n",
    "        result = model.generate(\n",
    "            input=tokenized,\n",
    "            max_new_tokens=50,\n",
    "            do_sample=True,\n",
    "            **kwargs)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_generate(example_prompt):\n",
    "  model.reset_hooks()\n",
    "  editing_hooks = [(f\"blocks.{layer}.hook_resid_pre\", steering_hook)]\n",
    "  res = hooked_generate([example_prompt] * 3, editing_hooks, seed=None, **sampling_kwargs)\n",
    "\n",
    "  # Print results, removing the ugly beginning of sequence token\n",
    "  res_str = model.to_string(res[:, 1:])\n",
    "  print((\"\\n\\n\" + \"-\" * 80 + \"\\n\\n\").join(res_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 77.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What do we find in space?\n",
      "\n",
      "The Jedi are the most common species of Jedi, and they are the only ones who have been known to be able to survive on a planet. They are not considered sentient by most people, but they can be found in many different forms.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "What do we find in space?\n",
      "\n",
      "We've been told that the Jedi are not only good at combat, but also have a great sense of humor. We've seen them play a lot of games and even read books about them. But what if they were just as good at\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "What do we find in space?\n",
      "\n",
      "We've been told that the Jedi are not as powerful as they appear. They're just more powerful than the Jedi, and they're also less intelligent than their Jedi counterparts. But what about the other races? The answer is simple: there\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "steering_on = True\n",
    "run_generate(example_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 91.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What do we find in space?\n",
      "\n",
      "The world is full of surprises. The first thing that comes to mind is the mysterious and mysterious nature of our universe. We are constantly looking for new things to explore, but there are so many unknowns that we can't even begin to\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "What do we find in space?\n",
      "\n",
      "The first thing we find is the density of matter. The density of matter is the ratio between the mass and mass of a substance. This means that if you have a liquid, it will be denser than water, but if you have\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "What do we find in space?\n",
      "\n",
      "The most common questions we get are: \"How does the Earth work?\" and \"How does it work on Mars?\" The answer is that the Earth is a very complex system. It has many different types of planets, moons, satellites,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "steering_on = False\n",
    "run_generate(example_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_prompt = \"What is on your mind?\"\n",
    "coeff = 100\n",
    "sampling_kwargs = dict(temperature=1.0, top_p=0.1, freq_penalty=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 84.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is on your mind?\n",
      "\n",
      "The Jedi are a group of Jedi who have been trained by the Sith Lord Darth Vader. They are known as the \"Dark Jedi\" and are considered to be among the most powerful in the galaxy. The Dark Jedi were created by Darth Vader\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "What is on your mind?\n",
      "\n",
      "I'm not sure if you're aware of the Jedi Order, but I've been a Jedi since I was a child. My parents were both Jedi and my father was an Imperial Knight. I remember being raised by my father as a boy\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "What is on your mind?\n",
      "\n",
      "If you're looking for a way to get started with the Jedi Order, then this is the place to start. The Jedi Order has been around since at least the Clone Wars and it's been a long time since we've seen any of\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "steering_on = True\n",
    "run_generate(question_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 90.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is on your mind?\n",
      "\n",
      "I'm not sure what I'm thinking. I've been reading a lot of books about the Bible and it's hard to find anything that's really relevant to me. So, I'm just going to try and get some information out there\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "What is on your mind?\n",
      "\n",
      "I'm not sure what I'm thinking. I've been thinking about this for a while now, and it's something that I think about a lot. It's something that I think about when you're in the middle of an argument with\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "What is on your mind?\n",
      "\n",
      "I'm not sure what I'm thinking. I've been thinking about this for a while now, and it's something that I've been trying to figure out for a while now. It's kind of like the \"what if\" question\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "steering_on = False\n",
    "run_generate(question_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xianxuan_sae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
